{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import PcapDataset\n",
    "from constants import PCAP_PATH\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import sys\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCAP_PATH = \"../data/benign/weekday.pcap\"\n",
    "CSV_PATH  = \"../data/benign/weekday.csv\"\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size) -> None:\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        trx = x.reshape(1, 1, x.shape[-1]).to(torch.float)\n",
    "        _, (hidden, _) = self.lstm(trx)  # Use the last hidden state for prediction\n",
    "        out = self.linear(hidden[-1])\n",
    "        return out\n",
    "# Instantiate the model with specified dimensions\n",
    "model = LSTM(input_size=235, hidden_size=64, num_layers=2, output_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s4716806\\AppData\\Local\\anaconda3\\envs\\panda\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([100])) that is different to the input size (torch.Size([1, 100])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 362702752.0000\n",
      "Epoch 2, Loss: 355382016.0000\n",
      "Epoch 3, Loss: 348202976.0000\n",
      "Epoch 4, Loss: 341162688.0000\n",
      "Epoch 5, Loss: 334259808.0000\n",
      "Epoch 6, Loss: 327492800.0000\n",
      "Epoch 7, Loss: 320860640.0000\n",
      "Epoch 8, Loss: 314359264.0000\n",
      "Epoch 9, Loss: 307986464.0000\n",
      "Epoch 10, Loss: 301741440.0000\n",
      "Epoch 11, Loss: 295623936.0000\n",
      "Epoch 12, Loss: 289627168.0000\n",
      "Epoch 13, Loss: 283752992.0000\n",
      "Epoch 14, Loss: 277993376.0000\n",
      "Epoch 15, Loss: 272354112.0000\n",
      "Epoch 16, Loss: 266831232.0000\n",
      "Epoch 17, Loss: 261420832.0000\n",
      "Epoch 18, Loss: 256123696.0000\n",
      "Epoch 19, Loss: 250938048.0000\n",
      "Epoch 20, Loss: 245861680.0000\n",
      "Epoch 21, Loss: 240891680.0000\n",
      "Epoch 22, Loss: 236027808.0000\n",
      "Epoch 23, Loss: 231268560.0000\n",
      "Epoch 24, Loss: 226612240.0000\n",
      "Epoch 25, Loss: 222056256.0000\n",
      "Epoch 26, Loss: 217599792.0000\n",
      "Epoch 27, Loss: 213239680.0000\n",
      "Epoch 28, Loss: 208976688.0000\n",
      "Epoch 29, Loss: 204808064.0000\n",
      "Epoch 30, Loss: 200730944.0000\n",
      "Epoch 31, Loss: 196744400.0000\n",
      "Epoch 32, Loss: 192846464.0000\n",
      "Epoch 33, Loss: 189037344.0000\n",
      "Epoch 34, Loss: 185315856.0000\n",
      "Epoch 35, Loss: 181677952.0000\n",
      "Epoch 36, Loss: 178123424.0000\n",
      "Epoch 37, Loss: 174654080.0000\n",
      "Epoch 38, Loss: 171259440.0000\n",
      "Epoch 39, Loss: 167944256.0000\n",
      "Epoch 40, Loss: 164709344.0000\n",
      "Epoch 41, Loss: 161540592.0000\n",
      "Epoch 42, Loss: 158449456.0000\n",
      "Epoch 43, Loss: 155420880.0000\n",
      "Epoch 44, Loss: 152456944.0000\n",
      "Epoch 45, Loss: 149566000.0000\n",
      "Epoch 46, Loss: 146734432.0000\n",
      "Epoch 47, Loss: 143978544.0000\n",
      "Epoch 48, Loss: 141291824.0000\n",
      "Epoch 49, Loss: 138647968.0000\n",
      "Epoch 50, Loss: 136071632.0000\n",
      "Epoch 51, Loss: 133559784.0000\n",
      "Epoch 52, Loss: 131103808.0000\n",
      "Epoch 53, Loss: 128698104.0000\n",
      "Epoch 54, Loss: 126358752.0000\n",
      "Epoch 55, Loss: 124073120.0000\n",
      "Epoch 56, Loss: 121840888.0000\n",
      "Epoch 57, Loss: 119668200.0000\n",
      "Epoch 58, Loss: 117544800.0000\n",
      "Epoch 59, Loss: 115476688.0000\n",
      "Epoch 60, Loss: 113468200.0000\n",
      "Epoch 61, Loss: 111503480.0000\n",
      "Epoch 62, Loss: 109589360.0000\n",
      "Epoch 63, Loss: 107727144.0000\n",
      "Epoch 64, Loss: 105920808.0000\n",
      "Epoch 65, Loss: 104158784.0000\n",
      "Epoch 66, Loss: 102442136.0000\n",
      "Epoch 67, Loss: 100776664.0000\n",
      "Epoch 68, Loss: 99155736.0000\n",
      "Epoch 69, Loss: 97588432.0000\n",
      "Epoch 70, Loss: 96064616.0000\n",
      "Epoch 71, Loss: 94583320.0000\n",
      "Epoch 72, Loss: 93146288.0000\n",
      "Epoch 73, Loss: 91752288.0000\n",
      "Epoch 74, Loss: 90409736.0000\n",
      "Epoch 75, Loss: 89111080.0000\n",
      "Epoch 76, Loss: 87844712.0000\n",
      "Epoch 77, Loss: 86621776.0000\n",
      "Epoch 78, Loss: 85436832.0000\n",
      "Epoch 79, Loss: 84286000.0000\n",
      "Epoch 80, Loss: 83185152.0000\n",
      "Epoch 81, Loss: 82122136.0000\n",
      "Epoch 82, Loss: 81095984.0000\n",
      "Epoch 83, Loss: 80106064.0000\n",
      "Epoch 84, Loss: 79152208.0000\n",
      "Epoch 85, Loss: 78237136.0000\n",
      "Epoch 86, Loss: 77356912.0000\n",
      "Epoch 87, Loss: 76510160.0000\n",
      "Epoch 88, Loss: 75698808.0000\n",
      "Epoch 89, Loss: 74921624.0000\n",
      "Epoch 90, Loss: 74175656.0000\n",
      "Epoch 91, Loss: 73463024.0000\n",
      "Epoch 92, Loss: 72785648.0000\n",
      "Epoch 93, Loss: 72138464.0000\n",
      "Epoch 94, Loss: 71524056.0000\n",
      "Epoch 95, Loss: 70935840.0000\n",
      "Epoch 96, Loss: 70383792.0000\n",
      "Epoch 97, Loss: 69860040.0000\n",
      "Epoch 98, Loss: 69365480.0000\n",
      "Epoch 99, Loss: 68902224.0000\n",
      "Epoch 100, Loss: 68463848.0000\n"
     ]
    }
   ],
   "source": [
    "# Load feature vectors from CSV file\n",
    "features = pd.read_csv(CSV_PATH).iloc[:, :-2]\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "for epoch in range(100):\n",
    "    # Create the dataset\n",
    "    dataset = PcapDataset(pcap_file=PCAP_PATH, max_iterations=sys.maxsize)\n",
    "\n",
    "    # Create the DataLoader\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "    for i, packets in enumerate(dataloader):\n",
    "        # Define loss function and optimizer\n",
    "        outputs = model(packets)\n",
    "        loss = criterion(outputs, torch.from_numpy(features.iloc[i].to_numpy()).to(torch.float))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print training progress\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 9.6000e+01, 0.0000e+00, 1.0000e+00, 9.6000e+01, 0.0000e+00,\n",
       "        1.0000e+00, 9.6000e+01, 0.0000e+00, 1.0000e+00, 9.6000e+01, 0.0000e+00,\n",
       "        1.0000e+00, 9.6000e+01, 0.0000e+00, 1.0000e+00, 9.6000e+01, 0.0000e+00,\n",
       "        0.0000e+00, 1.1015e+02, 0.0000e+00, 0.0000e+00, 1.0000e+00, 9.6000e+01,\n",
       "        0.0000e+00, 0.0000e+00, 1.1015e+02, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
       "        9.6000e+01, 0.0000e+00, 0.0000e+00, 1.1015e+02, 0.0000e+00, 0.0000e+00,\n",
       "        1.0000e+00, 9.6000e+01, 0.0000e+00, 9.0949e-13, 1.1015e+02, 0.0000e+00,\n",
       "        0.0000e+00, 1.0000e+00, 9.6000e+01, 0.0000e+00, 0.0000e+00, 1.1015e+02,\n",
       "        0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
       "        9.6000e+01, 0.0000e+00, 0.0000e+00, 9.6000e+01, 0.0000e+00, 0.0000e+00,\n",
       "        1.0000e+00, 9.6000e+01, 0.0000e+00, 0.0000e+00, 9.6000e+01, 0.0000e+00,\n",
       "        0.0000e+00, 1.0000e+00, 9.6000e+01, 0.0000e+00, 0.0000e+00, 9.6000e+01,\n",
       "        0.0000e+00, 0.0000e+00, 1.0000e+00, 9.6000e+01, 0.0000e+00, 0.0000e+00,\n",
       "        9.6000e+01, 0.0000e+00, 0.0000e+00, 1.0000e+00, 9.6000e+01, 0.0000e+00,\n",
       "        0.0000e+00, 9.6000e+01, 0.0000e+00, 0.0000e+00])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = pd.read_csv(CSV_PATH).iloc[:, :-2]\n",
    "torch.from_numpy(features.iloc[1].to_numpy()).to(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HT_MI_5_weight</th>\n",
       "      <th>HT_MI_5_mean</th>\n",
       "      <th>HT_MI_5_std</th>\n",
       "      <th>HT_MI_3_weight</th>\n",
       "      <th>HT_MI_3_mean</th>\n",
       "      <th>HT_MI_3_std</th>\n",
       "      <th>HT_MI_1_weight</th>\n",
       "      <th>HT_MI_1_mean</th>\n",
       "      <th>HT_MI_1_std</th>\n",
       "      <th>HT_MI_0.1_weight</th>\n",
       "      <th>...</th>\n",
       "      <th>HT_Hp_0.1_magnitude</th>\n",
       "      <th>HT_Hp_0.1_covariance</th>\n",
       "      <th>HT_Hp_0.1_pcc</th>\n",
       "      <th>HT_Hp_0.01_weight</th>\n",
       "      <th>HT_Hp_0.01_mean</th>\n",
       "      <th>HT_Hp_0.01_std</th>\n",
       "      <th>HT_Hp_0.01_radius</th>\n",
       "      <th>HT_Hp_0.01_magnitude</th>\n",
       "      <th>HT_Hp_0.01_covariance</th>\n",
       "      <th>HT_Hp_0.01_pcc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.999384</td>\n",
       "      <td>91.498613</td>\n",
       "      <td>20.249998</td>\n",
       "      <td>1.999630</td>\n",
       "      <td>91.499168</td>\n",
       "      <td>20.249999</td>\n",
       "      <td>1.999877</td>\n",
       "      <td>91.499723</td>\n",
       "      <td>20.250000</td>\n",
       "      <td>1.999988</td>\n",
       "      <td>...</td>\n",
       "      <td>91.499972</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.999999</td>\n",
       "      <td>91.499997</td>\n",
       "      <td>20.250000</td>\n",
       "      <td>20.250000</td>\n",
       "      <td>91.499997</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.917074</td>\n",
       "      <td>64.954193</td>\n",
       "      <td>110.043709</td>\n",
       "      <td>1.949386</td>\n",
       "      <td>64.772624</td>\n",
       "      <td>110.175676</td>\n",
       "      <td>1.982836</td>\n",
       "      <td>64.590893</td>\n",
       "      <td>110.241738</td>\n",
       "      <td>1.998270</td>\n",
       "      <td>...</td>\n",
       "      <td>118.309953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.250000</td>\n",
       "      <td>118.309972</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.835460</td>\n",
       "      <td>71.671202</td>\n",
       "      <td>154.046528</td>\n",
       "      <td>2.899159</td>\n",
       "      <td>71.404677</td>\n",
       "      <td>155.705889</td>\n",
       "      <td>2.965658</td>\n",
       "      <td>71.135514</td>\n",
       "      <td>157.262148</td>\n",
       "      <td>2.996532</td>\n",
       "      <td>...</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>64.390300</td>\n",
       "      <td>58.551943</td>\n",
       "      <td>1444.491701</td>\n",
       "      <td>71.245826</td>\n",
       "      <td>61.677086</td>\n",
       "      <td>3357.467390</td>\n",
       "      <td>85.315213</td>\n",
       "      <td>88.203451</td>\n",
       "      <td>31338.731422</td>\n",
       "      <td>155.374384</td>\n",
       "      <td>...</td>\n",
       "      <td>1456.919742</td>\n",
       "      <td>-1125.076738</td>\n",
       "      <td>-0.090714</td>\n",
       "      <td>76.962076</td>\n",
       "      <td>64.486558</td>\n",
       "      <td>4025.863982</td>\n",
       "      <td>40551.972963</td>\n",
       "      <td>1455.855829</td>\n",
       "      <td>-4200.761291</td>\n",
       "      <td>-0.329585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>65.320267</td>\n",
       "      <td>58.482256</td>\n",
       "      <td>1422.690064</td>\n",
       "      <td>72.199322</td>\n",
       "      <td>61.570754</td>\n",
       "      <td>3311.769647</td>\n",
       "      <td>86.296647</td>\n",
       "      <td>87.807104</td>\n",
       "      <td>30988.979572</td>\n",
       "      <td>156.371003</td>\n",
       "      <td>...</td>\n",
       "      <td>1456.913885</td>\n",
       "      <td>-1122.440902</td>\n",
       "      <td>-0.091074</td>\n",
       "      <td>77.961908</td>\n",
       "      <td>64.352049</td>\n",
       "      <td>3975.617564</td>\n",
       "      <td>40547.015494</td>\n",
       "      <td>1455.849877</td>\n",
       "      <td>-4188.699383</td>\n",
       "      <td>-0.330709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>66.272302</td>\n",
       "      <td>58.414622</td>\n",
       "      <td>1401.521302</td>\n",
       "      <td>73.167508</td>\n",
       "      <td>61.467283</td>\n",
       "      <td>3267.279452</td>\n",
       "      <td>87.283969</td>\n",
       "      <td>87.419780</td>\n",
       "      <td>30646.887504</td>\n",
       "      <td>157.368705</td>\n",
       "      <td>...</td>\n",
       "      <td>1456.908190</td>\n",
       "      <td>-1119.810009</td>\n",
       "      <td>-0.091428</td>\n",
       "      <td>78.961793</td>\n",
       "      <td>64.220947</td>\n",
       "      <td>3926.608927</td>\n",
       "      <td>40542.239555</td>\n",
       "      <td>1455.844088</td>\n",
       "      <td>-4176.699969</td>\n",
       "      <td>-0.331813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>67.238578</td>\n",
       "      <td>58.348966</td>\n",
       "      <td>1380.962832</td>\n",
       "      <td>74.145166</td>\n",
       "      <td>61.366571</td>\n",
       "      <td>3223.955371</td>\n",
       "      <td>88.275084</td>\n",
       "      <td>87.041194</td>\n",
       "      <td>30312.221632</td>\n",
       "      <td>158.367103</td>\n",
       "      <td>...</td>\n",
       "      <td>1456.902648</td>\n",
       "      <td>-1117.184313</td>\n",
       "      <td>-0.091777</td>\n",
       "      <td>79.961713</td>\n",
       "      <td>64.093124</td>\n",
       "      <td>3878.792948</td>\n",
       "      <td>40537.636403</td>\n",
       "      <td>1455.838455</td>\n",
       "      <td>-4164.762776</td>\n",
       "      <td>-0.332898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>204.978781</td>\n",
       "      <td>1471.337290</td>\n",
       "      <td>17449.840889</td>\n",
       "      <td>218.193356</td>\n",
       "      <td>1466.525953</td>\n",
       "      <td>24017.461670</td>\n",
       "      <td>232.938521</td>\n",
       "      <td>1459.641027</td>\n",
       "      <td>33350.245177</td>\n",
       "      <td>240.170676</td>\n",
       "      <td>...</td>\n",
       "      <td>1457.021201</td>\n",
       "      <td>-1114.571229</td>\n",
       "      <td>-0.091750</td>\n",
       "      <td>245.922117</td>\n",
       "      <td>1454.547179</td>\n",
       "      <td>40191.099793</td>\n",
       "      <td>40377.834728</td>\n",
       "      <td>1455.958593</td>\n",
       "      <td>-4152.895061</td>\n",
       "      <td>-0.332612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       HT_MI_5_weight  HT_MI_5_mean   HT_MI_5_std  HT_MI_3_weight  \\\n",
       "0            1.000000     54.000000      0.000000        1.000000   \n",
       "1            1.000000     96.000000      0.000000        1.000000   \n",
       "2            1.999384     91.498613     20.249998        1.999630   \n",
       "3            1.917074     64.954193    110.043709        1.949386   \n",
       "4            2.835460     71.671202    154.046528        2.899159   \n",
       "...               ...           ...           ...             ...   \n",
       "19995       64.390300     58.551943   1444.491701       71.245826   \n",
       "19996       65.320267     58.482256   1422.690064       72.199322   \n",
       "19997       66.272302     58.414622   1401.521302       73.167508   \n",
       "19998       67.238578     58.348966   1380.962832       74.145166   \n",
       "19999      204.978781   1471.337290  17449.840889      218.193356   \n",
       "\n",
       "       HT_MI_3_mean   HT_MI_3_std  HT_MI_1_weight  HT_MI_1_mean   HT_MI_1_std  \\\n",
       "0         54.000000      0.000000        1.000000     54.000000      0.000000   \n",
       "1         96.000000      0.000000        1.000000     96.000000      0.000000   \n",
       "2         91.499168     20.249999        1.999877     91.499723     20.250000   \n",
       "3         64.772624    110.175676        1.982836     64.590893    110.241738   \n",
       "4         71.404677    155.705889        2.965658     71.135514    157.262148   \n",
       "...             ...           ...             ...           ...           ...   \n",
       "19995     61.677086   3357.467390       85.315213     88.203451  31338.731422   \n",
       "19996     61.570754   3311.769647       86.296647     87.807104  30988.979572   \n",
       "19997     61.467283   3267.279452       87.283969     87.419780  30646.887504   \n",
       "19998     61.366571   3223.955371       88.275084     87.041194  30312.221632   \n",
       "19999   1466.525953  24017.461670      232.938521   1459.641027  33350.245177   \n",
       "\n",
       "       HT_MI_0.1_weight  ...  HT_Hp_0.1_magnitude  HT_Hp_0.1_covariance  \\\n",
       "0              1.000000  ...            54.000000              0.000000   \n",
       "1              1.000000  ...            96.000000              0.000000   \n",
       "2              1.999988  ...            91.499972              0.000000   \n",
       "3              1.998270  ...           118.309953              0.000000   \n",
       "4              2.996532  ...            84.000000              0.000000   \n",
       "...                 ...  ...                  ...                   ...   \n",
       "19995        155.374384  ...          1456.919742          -1125.076738   \n",
       "19996        156.371003  ...          1456.913885          -1122.440902   \n",
       "19997        157.368705  ...          1456.908190          -1119.810009   \n",
       "19998        158.367103  ...          1456.902648          -1117.184313   \n",
       "19999        240.170676  ...          1457.021201          -1114.571229   \n",
       "\n",
       "       HT_Hp_0.1_pcc  HT_Hp_0.01_weight  HT_Hp_0.01_mean  HT_Hp_0.01_std  \\\n",
       "0           0.000000           1.000000        54.000000        0.000000   \n",
       "1           0.000000           1.000000        96.000000        0.000000   \n",
       "2           0.000000           1.999999        91.499997       20.250000   \n",
       "3           0.000000           1.000000        75.000000        0.000000   \n",
       "4           0.000000           1.000000        84.000000        0.000000   \n",
       "...              ...                ...              ...             ...   \n",
       "19995      -0.090714          76.962076        64.486558     4025.863982   \n",
       "19996      -0.091074          77.961908        64.352049     3975.617564   \n",
       "19997      -0.091428          78.961793        64.220947     3926.608927   \n",
       "19998      -0.091777          79.961713        64.093124     3878.792948   \n",
       "19999      -0.091750         245.922117      1454.547179    40191.099793   \n",
       "\n",
       "       HT_Hp_0.01_radius  HT_Hp_0.01_magnitude  HT_Hp_0.01_covariance  \\\n",
       "0               0.000000             54.000000               0.000000   \n",
       "1               0.000000             96.000000               0.000000   \n",
       "2              20.250000             91.499997               0.000000   \n",
       "3              20.250000            118.309972               0.000000   \n",
       "4               0.000000             84.000000               0.000000   \n",
       "...                  ...                   ...                    ...   \n",
       "19995       40551.972963           1455.855829           -4200.761291   \n",
       "19996       40547.015494           1455.849877           -4188.699383   \n",
       "19997       40542.239555           1455.844088           -4176.699969   \n",
       "19998       40537.636403           1455.838455           -4164.762776   \n",
       "19999       40377.834728           1455.958593           -4152.895061   \n",
       "\n",
       "       HT_Hp_0.01_pcc  \n",
       "0            0.000000  \n",
       "1            0.000000  \n",
       "2            0.000000  \n",
       "3            0.000000  \n",
       "4            0.000000  \n",
       "...               ...  \n",
       "19995       -0.329585  \n",
       "19996       -0.330709  \n",
       "19997       -0.331813  \n",
       "19998       -0.332898  \n",
       "19999       -0.332612  \n",
       "\n",
       "[20000 rows x 100 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0]\n",
      "The gradient with respect to 10 is 20.0\n",
      "The gradients with respect to each binary digit of 10 are [0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def binary_gradient(n):\n",
    "    # Convert n to binary and reverse it\n",
    "    binary = list(map(int, bin(n)[2:]))[::-1]\n",
    "\n",
    "    # Convert each digit to a PyTorch tensor\n",
    "    binary_tensors = [torch.tensor(float(b), requires_grad=True) for b in binary]\n",
    "\n",
    "    # Compute the value of n as a function of the binary digits\n",
    "    n_tensor = sum(b * 2**i for i, b in enumerate(binary_tensors))\n",
    "\n",
    "    # Detach n_tensor from the computation graph and set requires_grad=True\n",
    "    n_tensor = n_tensor.detach().requires_grad_(True)\n",
    "\n",
    "    # Define the function\n",
    "    f = n_tensor ** 2\n",
    "\n",
    "    # Compute the gradient with respect to n\n",
    "    f.backward()\n",
    "\n",
    "    # The gradient with respect to n\n",
    "    n_grad = n_tensor.grad.item()\n",
    "\n",
    "    # The gradient with respect to each binary digit\n",
    "    # binary_grads = [b.grad.item() for b in binary_tensors]\n",
    "    binary_grads = [b.grad.item() if b.grad is not None else 0 for b in binary_tensors]\n",
    "    print(binary_grads)\n",
    "\n",
    "    return n_grad, binary_grads\n",
    "\n",
    "# Use the function\n",
    "n = 10\n",
    "n_grad, binary_grads = binary_gradient(n)\n",
    "print(f\"The gradient with respect to {n} is {n_grad}\")\n",
    "print(f\"The gradients with respect to each binary digit of {n} are {binary_grads}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Wireshark is installed, but cannot read manuf !\n"
     ]
    }
   ],
   "source": [
    "# read the pcap file\n",
    "import scapy.all as scapy\n",
    "import pandas as pd\n",
    "\n",
    "PCAP_PATH = \"../../data/malicious/Port_Scanning_SmartTV.pcap\"\n",
    "# Read the pcap file\n",
    "packets = scapy.rdpcap(PCAP_PATH)\n",
    "\n",
    "# crop to first 500 packets\n",
    "packets = packets[:500]\n",
    "\n",
    "# save pcap file\n",
    "scapy.wrpcap(\"../../data/malicious/Port_Scanning_SmartTV_500.pcap\", packets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 ('panda')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e906eac90d4759eceadef4a65cefdf038c0b6e825d313cb568d551d181dcba05"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
